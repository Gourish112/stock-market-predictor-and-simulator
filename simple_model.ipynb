{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= SIMPLIFIED STOCK PREDICTION MODEL =======\n",
      "Step 1: Loading and preprocessing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded 3823 days of data\n",
      "Data shape after preprocessing: (3794, 12)\n",
      "\n",
      "Step 2: Preparing data for time series modeling...\n",
      "Training set: 3035 samples\n",
      "Test set: 759 samples\n",
      "\n",
      "===== Building model for 1-day prediction =====\n",
      "Training sequences: (3014, 20, 11)\n",
      "Testing sequences: (738, 20, 11)\n",
      "Training model for 1-day...\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\Gourish\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:205: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 0.6952 - mae: 0.5833 - val_loss: 2.0357 - val_mae: 0.8739\n",
      "Epoch 2/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.6844 - mae: 0.5744 - val_loss: 2.0380 - val_mae: 0.8734\n",
      "Epoch 3/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.6823 - mae: 0.5720 - val_loss: 2.0429 - val_mae: 0.8771\n",
      "Epoch 4/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.6809 - mae: 0.5716 - val_loss: 2.0361 - val_mae: 0.8713\n",
      "Epoch 5/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.6825 - mae: 0.5718 - val_loss: 2.0361 - val_mae: 0.8743\n",
      "Epoch 6/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.6796 - mae: 0.5715 - val_loss: 2.0371 - val_mae: 0.8761\n",
      "Epoch 7/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.6781 - mae: 0.5703 - val_loss: 2.0312 - val_mae: 0.8680\n",
      "Epoch 8/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.6797 - mae: 0.5716 - val_loss: 2.0306 - val_mae: 0.8666\n",
      "Epoch 9/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.6776 - mae: 0.5699 - val_loss: 2.0312 - val_mae: 0.8682\n",
      "Epoch 10/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.6802 - mae: 0.5715 - val_loss: 2.0338 - val_mae: 0.8682\n",
      "Epoch 11/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.6756 - mae: 0.5702 - val_loss: 2.0304 - val_mae: 0.8642\n",
      "Epoch 12/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.6793 - mae: 0.5718 - val_loss: 2.0348 - val_mae: 0.8678\n",
      "Epoch 13/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.6763 - mae: 0.5703 - val_loss: 2.0367 - val_mae: 0.8693\n",
      "Epoch 14/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.6777 - mae: 0.5711 - val_loss: 2.0317 - val_mae: 0.8687\n",
      "Epoch 15/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.6763 - mae: 0.5718 - val_loss: 2.0348 - val_mae: 0.8685\n",
      "Epoch 16/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.6713 - mae: 0.5710 - val_loss: 2.0295 - val_mae: 0.8687\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 320\u001b[39m\n\u001b[32m    318\u001b[39m results = []\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m horizon_days, horizon_name \u001b[38;5;129;01min\u001b[39;00m horizons:\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m     result = \u001b[43mbuild_model_for_horizon\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhorizon_days\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhorizon_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[32m    322\u001b[39m         results.append(result)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 170\u001b[39m, in \u001b[36mbuild_model_for_horizon\u001b[39m\u001b[34m(horizon, horizon_name)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m    169\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining model for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhorizon_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m    177\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[38;5;66;03m# Plot training history\u001b[39;00m\n\u001b[32m    180\u001b[39m plt.figure(figsize=(\u001b[32m10\u001b[39m, \u001b[32m5\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gourish\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:118\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    116\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    120\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gourish\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:322\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m epoch_iterator.catch_stop_iteration():\n\u001b[32m    321\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator.enumerate_epoch():\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m         \u001b[43mcallbacks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mon_train_batch_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m         logs = \u001b[38;5;28mself\u001b[39m.train_function(iterator)\n\u001b[32m    324\u001b[39m         callbacks.on_train_batch_end(\n\u001b[32m    325\u001b[39m             step, \u001b[38;5;28mself\u001b[39m._pythonify_logs(logs)\n\u001b[32m    326\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gourish\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\callbacks\\callback_list.py:99\u001b[39m, in \u001b[36mCallbackList.on_train_batch_begin\u001b[39m\u001b[34m(self, batch, logs)\u001b[39m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callbacks:\n\u001b[32m     97\u001b[39m         callback.on_epoch_end(epoch, logs)\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mon_train_batch_begin\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    100\u001b[39m     logs = logs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callbacks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import yfinance as yf\n",
    "import time\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create output directory\n",
    "model_dir = \"stock_models_improved\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "print(\"======= SIMPLIFIED STOCK PREDICTION MODEL =======\")\n",
    "print(\"Step 1: Loading and preprocessing data...\")\n",
    "\n",
    "# Load stock data\n",
    "try:\n",
    "    start = dt.datetime(2010, 1, 1)\n",
    "    end = dt.datetime.today()\n",
    "    \n",
    "    # Download data with retry mechanism\n",
    "    max_attempts = 3\n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            df = yf.download(tickers=['^GSPC'], start=start, end=end)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            if attempt < max_attempts - 1:\n",
    "                print(f\"Attempt {attempt+1} failed. Retrying...\")\n",
    "                time.sleep(2)\n",
    "            else:\n",
    "                raise Exception(f\"Failed to download data after {max_attempts} attempts: {e}\")\n",
    "    \n",
    "    if df.empty:\n",
    "        raise Exception(\"Downloaded dataframe is empty\")\n",
    "    \n",
    "    print(f\"Successfully downloaded {len(df)} days of data\")\n",
    "    \n",
    "    # Basic preprocessing\n",
    "    df = df.dropna().reset_index()\n",
    "    \n",
    "    # Calculate returns instead of using raw prices\n",
    "    df['return'] = df['Close'].pct_change()\n",
    "    df['log_return'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "    \n",
    "    # Calculate volatility (20-day rolling standard deviation)\n",
    "    df['volatility'] = df['log_return'].rolling(window=20).std()\n",
    "    \n",
    "    # Calculate moving averages\n",
    "    df['sma_10'] = df['Close'].rolling(window=10).mean()\n",
    "    df['sma_30'] = df['Close'].rolling(window=30).mean()\n",
    "    \n",
    "    # Calculate price momentum (rate of change)\n",
    "    df['momentum_5'] = df['Close'].pct_change(periods=5)\n",
    "    \n",
    "    # Drop NaN values\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Data shape after preprocessing: {df.shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in data loading: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "print(\"\\nStep 2: Preparing data for time series modeling...\")\n",
    "\n",
    "try:\n",
    "    # Focus on predicting returns instead of absolute prices\n",
    "    # This makes the prediction task more realistic and normalizes the target variable\n",
    "    target_variable = 'return'\n",
    "    \n",
    "    # Select a reasonable set of features\n",
    "    features = ['return', 'log_return', 'volatility', \n",
    "                'sma_10', 'sma_30', 'momentum_5',\n",
    "                'Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    \n",
    "    # Separate test set (last 20% of data)\n",
    "    test_split = int(len(df) * 0.8)\n",
    "    train_df = df.iloc[:test_split].copy()\n",
    "    test_df = df.iloc[test_split:].copy()\n",
    "    \n",
    "    print(f\"Training set: {len(train_df)} samples\")\n",
    "    print(f\"Test set: {len(test_df)} samples\")\n",
    "    \n",
    "    # Function to create sequences - specialized for each prediction horizon\n",
    "    def create_sequences(data, features, target_col, seq_length, horizon):\n",
    "        X, y = [], []\n",
    "        feature_data = data[features].values\n",
    "        target_data = data[target_col].values\n",
    "        \n",
    "        for i in range(seq_length, len(data) - horizon):\n",
    "            X.append(feature_data[i-seq_length:i])\n",
    "            # Target is the return value 'horizon' days ahead\n",
    "            y.append(target_data[i + horizon - 1])\n",
    "            \n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in data preparation: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Function for building, training and evaluating models for each horizon\n",
    "def build_model_for_horizon(horizon, horizon_name):\n",
    "    print(f\"\\n===== Building model for {horizon_name} prediction =====\")\n",
    "    \n",
    "    try:\n",
    "        # Parameters\n",
    "        sequence_length = 20  # Use 20 days of data to predict\n",
    "        \n",
    "        # Create scaled datasets - scale each feature independently\n",
    "        scaler_dict = {}\n",
    "        scaled_train_data = train_df[features].copy()\n",
    "        scaled_test_data = test_df[features].copy()\n",
    "        \n",
    "        for feature in features:\n",
    "            scaler = StandardScaler()\n",
    "            scaled_train_data[feature] = scaler.fit_transform(train_df[feature].values.reshape(-1, 1))\n",
    "            scaled_test_data[feature] = scaler.transform(test_df[feature].values.reshape(-1, 1))\n",
    "            scaler_dict[feature] = scaler\n",
    "            \n",
    "        # Store target scaler for inverse transformation later\n",
    "        target_scaler = scaler_dict[target_variable]\n",
    "            \n",
    "        # Create sequences\n",
    "        X_train, y_train = create_sequences(\n",
    "            scaled_train_data, features, target_variable, sequence_length, horizon\n",
    "        )\n",
    "        \n",
    "        X_test, y_test = create_sequences(\n",
    "            scaled_test_data, features, target_variable, sequence_length, horizon\n",
    "        )\n",
    "        \n",
    "        print(f\"Training sequences: {X_train.shape}\")\n",
    "        print(f\"Testing sequences: {X_test.shape}\")\n",
    "        \n",
    "        # Build a simple but effective model\n",
    "        model = Sequential([\n",
    "            LSTM(50, activation='tanh', recurrent_activation='sigmoid', \n",
    "                 input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True),\n",
    "            Dropout(0.2),\n",
    "            LSTM(30, activation='tanh', recurrent_activation='sigmoid'),\n",
    "            Dropout(0.2),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        \n",
    "        # Compile with appropriate loss for returns prediction\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "        \n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "            ModelCheckpoint(\n",
    "                filepath=os.path.join(model_dir, f'model_{horizon_name.replace(\" \", \"_\")}.keras'),\n",
    "                monitor='val_loss', save_best_only=True\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Train the model\n",
    "        print(f\"Training model for {horizon_name}...\")\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=50,\n",
    "            batch_size=32,\n",
    "            validation_split=0.2,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Plot training history\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title(f'Training History - {horizon_name} Forecast')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(model_dir, f'training_history_{horizon_name.replace(\" \", \"_\")}.png'))\n",
    "        \n",
    "        # Evaluate the model\n",
    "        print(f\"\\nEvaluating {horizon_name} forecast model...\")\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Convert predictions back to original scale\n",
    "        y_test_orig = target_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "        y_pred_orig = target_scaler.inverse_transform(y_pred).flatten()\n",
    "        \n",
    "        # Calculate metrics on original scale data\n",
    "        mse = mean_squared_error(y_test_orig, y_pred_orig)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test_orig, y_pred_orig)\n",
    "        r2 = r2_score(y_test_orig, y_pred_orig)\n",
    "        \n",
    "        # Calculate MAPE carefully to handle zeros or near-zeros\n",
    "        mape = np.mean(np.abs((y_test_orig - y_pred_orig) / np.maximum(np.abs(y_test_orig), 1e-7))) * 100\n",
    "        \n",
    "        print(f\"\\n🔹 {horizon_name} Forecast Metrics:\")\n",
    "        print(f\"   • Mean Squared Error (MSE): {mse:.6f}\")\n",
    "        print(f\"   • Root Mean Squared Error (RMSE): {rmse:.6f}\")\n",
    "        print(f\"   • Mean Absolute Error (MAE): {mae:.6f}\")\n",
    "        print(f\"   • Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
    "        print(f\"   • R² Score: {r2:.4f}\")\n",
    "        \n",
    "        # Plot predictions vs actual returns\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(y_test_orig, label='Actual Returns', color='blue', alpha=0.6)\n",
    "        plt.plot(y_pred_orig, label='Predicted Returns', color='red', linestyle='--')\n",
    "        plt.title(f'{horizon_name} Return Prediction')\n",
    "        plt.xlabel('Trading Days')\n",
    "        plt.ylabel('Returns')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.savefig(os.path.join(model_dir, f'returns_prediction_{horizon_name.replace(\" \", \"_\")}.png'))\n",
    "        \n",
    "        # Convert returns to actual price predictions\n",
    "        last_prices = []\n",
    "        predicted_prices = []\n",
    "        actual_prices = []\n",
    "        \n",
    "        # Get price information\n",
    "        test_prices = test_df['Close'].values\n",
    "        \n",
    "        for i in range(len(y_test_orig)):\n",
    "            idx = i + sequence_length + horizon - 1\n",
    "            if idx < len(test_prices):\n",
    "                # For the first point, use the actual price as base\n",
    "                if i == 0:\n",
    "                    prev_price = test_prices[sequence_length - 1]\n",
    "                else:\n",
    "                    prev_price = test_prices[sequence_length + i - 2]\n",
    "                \n",
    "                # Actual price\n",
    "                actual_price = test_prices[idx]\n",
    "                \n",
    "                # Predicted price based on return prediction\n",
    "                predicted_return = y_pred_orig[i]\n",
    "                predicted_price = prev_price * (1 + predicted_return)\n",
    "                \n",
    "                last_prices.append(prev_price)\n",
    "                predicted_prices.append(predicted_price)\n",
    "                actual_prices.append(actual_price)\n",
    "        \n",
    "        # Plot price predictions\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(actual_prices, label='Actual Price', color='blue')\n",
    "        plt.plot(predicted_prices, label='Predicted Price', color='red', linestyle='--')\n",
    "        plt.title(f'{horizon_name} Price Prediction')\n",
    "        plt.xlabel('Trading Days')\n",
    "        plt.ylabel('Price ($)')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.savefig(os.path.join(model_dir, f'price_prediction_{horizon_name.replace(\" \", \"_\")}.png'))\n",
    "        \n",
    "        # Calculate price prediction metrics\n",
    "        price_mse = mean_squared_error(actual_prices, predicted_prices)\n",
    "        price_rmse = np.sqrt(price_mse)\n",
    "        price_mae = mean_absolute_error(actual_prices, predicted_prices)\n",
    "        price_mape = np.mean(np.abs((np.array(actual_prices) - np.array(predicted_prices)) / np.array(actual_prices))) * 100\n",
    "        price_r2 = r2_score(actual_prices, predicted_prices)\n",
    "        \n",
    "        print(f\"\\n🔹 {horizon_name} Price Prediction Metrics:\")\n",
    "        print(f\"   • Mean Squared Error (MSE): {price_mse:.4f}\")\n",
    "        print(f\"   • Root Mean Squared Error (RMSE): {price_rmse:.4f}\")\n",
    "        print(f\"   • Mean Absolute Error (MAE): {price_mae:.4f}\")\n",
    "        print(f\"   • Mean Absolute Percentage Error (MAPE): {price_mape:.2f}%\")\n",
    "        print(f\"   • R² Score: {price_r2:.4f}\")\n",
    "        \n",
    "        # Make future prediction\n",
    "        last_sequence = scaled_test_data[features].values[-sequence_length:]\n",
    "        last_sequence = last_sequence.reshape(1, sequence_length, len(features))\n",
    "        predicted_return = model.predict(last_sequence)[0][0]\n",
    "        \n",
    "        # Convert to original scale\n",
    "        predicted_return_orig = target_scaler.inverse_transform([[predicted_return]])[0][0]\n",
    "        \n",
    "        # Get the latest price\n",
    "        latest_price = df['Close'].iloc[-1]\n",
    "        \n",
    "        # Calculate predicted price\n",
    "        predicted_price = latest_price * (1 + predicted_return_orig)\n",
    "        \n",
    "        print(f\"\\n🔹 Future {horizon_name} Prediction:\")\n",
    "        print(f\"   • Current Price: ${latest_price:.2f}\")\n",
    "        print(f\"   • Predicted {horizon_name} Return: {predicted_return_orig*100:.2f}%\")\n",
    "        print(f\"   • Predicted {horizon_name} Price: ${predicted_price:.2f}\")\n",
    "        \n",
    "        # Return key metrics for comparison\n",
    "        return {\n",
    "            'horizon': horizon_name,\n",
    "            'return_r2': r2,\n",
    "            'price_r2': price_r2,\n",
    "            'price_mape': price_mape,\n",
    "            'model': model\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in {horizon_name} model: {e}\")\n",
    "        return None\n",
    "\n",
    "# Train models for different prediction horizons\n",
    "horizons = [\n",
    "    (1, \"1-day\"),\n",
    "    (5, \"1-week\"),\n",
    "    (20, \"1-month\")\n",
    "]\n",
    "\n",
    "results = []\n",
    "for horizon_days, horizon_name in horizons:\n",
    "    result = build_model_for_horizon(horizon_days, horizon_name)\n",
    "    if result:\n",
    "        results.append(result)\n",
    "\n",
    "# Compare model performance across horizons\n",
    "if results:\n",
    "    print(\"\\n===== MODEL COMPARISON =====\")\n",
    "    for result in results:\n",
    "        print(f\"🔹 {result['horizon']}:\")\n",
    "        print(f\"   • Return Prediction R²: {result['return_r2']:.4f}\")\n",
    "        print(f\"   • Price Prediction R²: {result['price_r2']:.4f}\")\n",
    "        print(f\"   • Price Prediction MAPE: {result['price_mape']:.2f}%\")\n",
    "\n",
    "print(\"\\nModel training and evaluation complete. Check the 'stock_models_improved' directory for outputs.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
