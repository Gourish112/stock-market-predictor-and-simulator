{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2bbd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import requests\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from fredapi import Fred\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Bidirectional, Flatten, Input, LayerNormalization, MultiHeadAttention, Add, GlobalAveragePooling1D, Attention\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import ta\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# === API Keys ===\n",
    "NEWS_API_KEY = \"e7b02a6bae394d849381d33ee450abf4\"\n",
    "FRED_API_KEY = \"\"\n",
    "fred = Fred(api_key=FRED_API_KEY)\n",
    "\n",
    "# === Load Macroeconomic Data ===\n",
    "def load_macro_data(start_date, end_date):\n",
    "    indicators = {\n",
    "        'FEDFUNDS': 'interest_rate',\n",
    "        'CPIAUCSL': 'cpi',\n",
    "        'INDPRO': 'industrial_production',\n",
    "        'UNRATE': 'unemployment_rate'\n",
    "    }\n",
    "\n",
    "    macro_df = pd.DataFrame()\n",
    "    for code, name in indicators.items():\n",
    "        data = fred.get_series(code, start_date, end_date)\n",
    "        macro_df[name] = data\n",
    "\n",
    "    macro_df.index = pd.to_datetime(macro_df.index)\n",
    "    macro_df = macro_df.resample('D').ffill()\n",
    "    return macro_df\n",
    "\n",
    "# === News Sentiment ===\n",
    "def fetch_news_sentiment(stock_symbol, start_date, end_date):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    url = (\n",
    "        f'https://newsapi.org/v2/everything?'\n",
    "        f'q={stock_symbol}&'\n",
    "        f'from={start_date}&'\n",
    "        f'to={end_date}&'\n",
    "        f'sortBy=publishedAt&'\n",
    "        f'apiKey={NEWS_API_KEY}'\n",
    "    )\n",
    "    response = requests.get(url)\n",
    "    articles = response.json().get('articles', [])\n",
    "\n",
    "    news_list = []\n",
    "    for article in articles:\n",
    "        date = pd.to_datetime(article['publishedAt']).date()\n",
    "        title = article['title'] or \"\"\n",
    "        sentiment = analyzer.polarity_scores(title)['compound']\n",
    "        news_list.append({'date': date, 'sentiment': sentiment})\n",
    "\n",
    "    news_df = pd.DataFrame(news_list)\n",
    "    if news_df.empty:\n",
    "        return pd.Series(dtype='float64')\n",
    "\n",
    "    daily_sentiment = news_df.groupby('date')['sentiment'].mean()\n",
    "    daily_sentiment.index = pd.to_datetime(daily_sentiment.index)\n",
    "    return daily_sentiment.resample('D').ffill()\n",
    "\n",
    "# === Load Stock + Merge All ===\n",
    "def load_data(stock_symbol, start_date, end_date):\n",
    "    stock_df = yf.download(stock_symbol, start=start_date, end=end_date)\n",
    "    stock_df.dropna(inplace=True)\n",
    "\n",
    "    stock_df['rsi'] = ta.momentum.RSIIndicator(stock_df['Close']).rsi()\n",
    "    stock_df['macd'] = ta.trend.MACD(stock_df['Close']).macd_diff()\n",
    "    stock_df['sma'] = ta.trend.SMAIndicator(stock_df['Close']).sma_indicator()\n",
    "    stock_df['ema'] = ta.trend.EMAIndicator(stock_df['Close']).ema_indicator()\n",
    "\n",
    "    macro_df = load_macro_data(start_date, end_date)\n",
    "    news_sentiment = fetch_news_sentiment(stock_symbol, start_date, end_date)\n",
    "    stock_df['news_sentiment'] = news_sentiment\n",
    "\n",
    "    data = stock_df[['Close', 'rsi', 'macd', 'sma', 'ema']].merge(\n",
    "        macro_df, left_index=True, right_index=True, how='left')\n",
    "    data['news_sentiment'] = stock_df['news_sentiment']\n",
    "    data.fillna(method='ffill', inplace=True)\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "    return data_scaled, scaler, data\n",
    "\n",
    "# === Create Dataset ===\n",
    "def create_dataset(data, time_step, forecast_horizon):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_step - forecast_horizon):\n",
    "        X.append(data[i:(i + time_step)])\n",
    "        if forecast_horizon == 1:\n",
    "            y.append(data[i + time_step, 0])\n",
    "        else:\n",
    "            y.append(np.mean(data[i + time_step:i + time_step + forecast_horizon, 0]))\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# === Model Architectures ===\n",
    "def create_1day_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv1D(64, kernel_size=3, activation='relu')(inputs)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Bidirectional(LSTM(64))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(1)(x)\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "def create_1week_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True))(inputs)\n",
    "    x = Attention()([x, x])\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(1)(x)\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "def create_1month_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = LSTM(64, return_sequences=True)(inputs)\n",
    "    x = LSTM(64, return_sequences=True)(x)\n",
    "    x = Attention()([x, x])\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(1)(x)\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "def create_1year_model(input_shape, num_heads=2, ff_dim=64):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv1D(64, kernel_size=3, activation='relu', padding='same')(inputs)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=64)(x, x)\n",
    "    x = Add()([x, attention_output])\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    x = Dense(ff_dim, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(1)(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    return Model(inputs, x)\n",
    "\n",
    "# === Evaluation ===\n",
    "def evaluate_model(model, X_test, y_test, scaler, num_features, name):\n",
    "    predicted = model.predict(X_test)\n",
    "    y_test_reshaped = y_test.reshape(-1, 1)\n",
    "    predicted_full = np.zeros((len(predicted), num_features))\n",
    "    actual_full = np.zeros((len(y_test), num_features))\n",
    "    predicted_full[:, 0] = predicted.flatten()\n",
    "    actual_full[:, 0] = y_test_reshaped.flatten()\n",
    "\n",
    "    predicted_prices = scaler.inverse_transform(predicted_full)[:, 0]\n",
    "    actual_prices = scaler.inverse_transform(actual_full)[:, 0]\n",
    "\n",
    "    print(f\"{name} MSE: {mean_squared_error(actual_prices, predicted_prices):.4f}\")\n",
    "    print(f\"{name} MAE: {mean_absolute_error(actual_prices, predicted_prices):.4f}\")\n",
    "    print(f\"{name} R2: {r2_score(actual_prices, predicted_prices):.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(actual_prices, label='Actual')\n",
    "    plt.plot(predicted_prices, label='Predicted')\n",
    "    plt.title(f'{name} Prediction')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# === Main ===\n",
    "if __name__ == '__main__':\n",
    "    stock = 'AAPL'\n",
    "    start = '2010-01-01'\n",
    "    end = '2024-12-31'\n",
    "    time_step = 60\n",
    "\n",
    "    data, scaler, processed_df = load_data(stock, start, end)\n",
    "    num_features = data.shape[1]\n",
    "\n",
    "    forecast_settings = {\n",
    "        '1-Day': {'horizon': 1, 'model_fn': create_1day_model},\n",
    "        '1-Week': {'horizon': 7, 'model_fn': create_1week_model},\n",
    "        '1-Month': {'horizon': 30, 'model_fn': create_1month_model},\n",
    "        '1-Year': {'horizon': 252, 'model_fn': create_1year_model}\n",
    "    }\n",
    "\n",
    "    for name, settings in forecast_settings.items():\n",
    "        print(f\"\\n=== Training {name} Model ===\")\n",
    "        X, y = create_dataset(data, time_step, settings['horizon'])\n",
    "\n",
    "        split = int(len(X) * 0.8)\n",
    "        X_train, X_test = X[:split], X[split:]\n",
    "        y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "        model = settings['model_fn'](X_train.shape[1:])\n",
    "        model.compile(optimizer=Adam(0.001), loss='mean_squared_error')\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "        model.fit(X_train, y_train, epochs=20, batch_size=64,\n",
    "                  validation_data=(X_test, y_test), callbacks=[early_stop], verbose=1)\n",
    "\n",
    "        evaluate_model(model, X_test, y_test, scaler, num_features, name)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
